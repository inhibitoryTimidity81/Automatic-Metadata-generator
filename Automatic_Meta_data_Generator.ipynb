{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e053bb0-c8ae-4e08-aee6-28cfd37fac29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT METADATA\n",
      "File: Books_CFD.jpg\n",
      "Extracted: 2025-06-25 21:32:41\n",
      "Type: General Document\n",
      "Pages: 1\n",
      "Words: 79\n",
      "Characters: 535\n",
      "\n",
      "Title:\n",
      "¢ P. S. Ghoshdastidar, “Computational Fluld\n",
      "\n",
      "Keywords:\n",
      "   computational, fluid, dynamics, heat, transfer, delhi, flow, suggested, books, ghoshdastidar\n",
      "\n",
      "Summary:\n",
      "Ghoshdastidar, “Computational Fluld\n",
      "Dynamics and Heat Transfer’, Cengage\n",
      "Learning India Pvt. Date, “Introduction to Computational Fluid\n",
      "Dynamics’, Cambridge Univ.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from skimage import measure\n",
    "import os\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import tempfile\n",
    "import shutil\n",
    "from pdf2image import convert_from_path\n",
    "import pymupdf as fitz\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import aspose.words as aw\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "class UniversalOCRProcessor:\n",
    "    def __init__(self):\n",
    "        #It supports image, pdfs, docs and txt files. \n",
    "        self.supported_extensions={'image': ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif'],\n",
    "                                  'pdf': ['.pdf'], 'docx': ['.docx', '.doc'], 'text': ['.txt'] }\n",
    "\n",
    "    # detecting the uploaded file type\n",
    "    def detect_file_type(self, file_path):\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found!\")\n",
    "        _, ext=os.path.splitext(file_path.lower())\n",
    "        for file_type, exts in self.supported_extensions.items():\n",
    "            if ext in exts:\n",
    "                return file_type\n",
    "        return 'unsupported'\n",
    "\n",
    "    #pdf to images\n",
    "    def pdf_to_images(self, pdf_path, dpi=300):\n",
    "        return convert_from_path(pdf_path, dpi=dpi)\n",
    "\n",
    "    #docs to images\n",
    "    def docx_to_images(self, docx_path, dpi=300):\n",
    "        doc=aw.Document(docx_path)\n",
    "        images=[]\n",
    "        #Rendering each page to PNG.\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            for i in range(doc.page_count):\n",
    "                out=os.path.join(temp_dir, f\"page_{i}.png\")\n",
    "                opts=aw.saving.ImageSaveOptions(aw.SaveFormat.PNG)\n",
    "                opts.page_set=aw.saving.PageSet(i)\n",
    "                opts.resolution=dpi\n",
    "                doc.save(out, opts)\n",
    "                images.append(Image.open(out).copy())\n",
    "        return images\n",
    "\n",
    "    #create an image from text.\n",
    "    def create_text_image(self, text, width=800, height=1000):\n",
    "        \n",
    "        img = Image.new('RGB', (width, height), color='white')\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        y_position = 20\n",
    "        line_height = 25\n",
    "        \n",
    "        for line in lines:\n",
    "            if y_position > height - 50:\n",
    "                break\n",
    "            draw.text((20, y_position), line, fill='black', font=font)\n",
    "            y_position += line_height\n",
    "        \n",
    "        return img\n",
    "        \n",
    "    #txt to image.\n",
    "    def txt_to_images(self, txt_path):\n",
    "            with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "                text_content = file.read()\n",
    "            \n",
    "            max_chars_per_page = 2000\n",
    "            pages = []\n",
    "            \n",
    "            if len(text_content) <= max_chars_per_page:\n",
    "                pages = [text_content]\n",
    "            else:\n",
    "                words = text_content.split(' ')\n",
    "                current_page = \"\"\n",
    "                \n",
    "                for word in words:\n",
    "                    if len(current_page + word) < max_chars_per_page:\n",
    "                        current_page += word + \" \"\n",
    "                    else:\n",
    "                        pages.append(current_page)\n",
    "                        current_page = word + \" \"\n",
    "                \n",
    "            if current_page:\n",
    "                pages.append(current_page)\n",
    "            \n",
    "            images = []\n",
    "            for page_text in pages:\n",
    "                img = self.create_text_image(page_text)\n",
    "                images.append(img)\n",
    "            \n",
    "            return images\n",
    "\n",
    "# Now the image processing will be done. All the images converted from different files\n",
    "# are processed using OpenCV (Rescaling, binarization etc)\n",
    "class OCRImageProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    #converts PIL into  OpenCV BGR and genrates gray scale version.\n",
    "    def set_image(self, image):\n",
    "        if isinstance(image, Image.Image):\n",
    "            #converts to BGR \n",
    "            self.original_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        else:\n",
    "            self.original_image = image\n",
    "        \n",
    "        self.processed_image = self.original_image.copy()\n",
    "        #gray scale version of original image.\n",
    "        self.gray_image = cv2.cvtColor(self.original_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    def rescale_image(self, image=None, scale_factor=2.0, interpolation=cv2.INTER_CUBIC):\n",
    "        if image is None:\n",
    "            image = self.gray_image\n",
    "        height, width = image.shape[:2]\n",
    "        new_width = int(width*scale_factor)\n",
    "        new_height = int(height*scale_factor)\n",
    "        rescaled = cv2.resize(image, (new_width, new_height), interpolation=interpolation)\n",
    "        return rescaled\n",
    "\n",
    "    #Binarize the image.\n",
    "    def binarize_image(self, image=None, method='otsu', threshold_value=127):\n",
    "        if image is None:\n",
    "            image =self.gray_image\n",
    "        if method=='otsu':\n",
    "            _, binary=cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid binarization method\")\n",
    "        \n",
    "        return binary\n",
    "\n",
    "    #denoising the image.\n",
    "    def remove_noise(self, image=None, method='median', kernel_size=5):\n",
    "        if image is None:\n",
    "            image=self.gray_image\n",
    "        \n",
    "        if method=='median':\n",
    "            denoised=cv2.medianBlur(image, kernel_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid noise removal method\")\n",
    "        return denoised\n",
    "\n",
    "    def remove_borders(self, image=None, border_size=10):\n",
    "        if image is None:\n",
    "            image = self.gray_image\n",
    "        h,w=image.shape[:2]\n",
    "        if h>border_size*2 and w> border_size*2:\n",
    "            cropped =image[border_size:h-border_size, border_size:w-border_size]\n",
    "            return cropped\n",
    "        return image\n",
    "    \n",
    "    def add_borders(self, image=None, border_size=20, border_color=255):\n",
    "        if image is None:\n",
    "            image=self.gray_image\n",
    "        bordered=cv2.copyMakeBorder(image, border_size, border_size, border_size, \n",
    "                                    border_size, cv2.BORDER_CONSTANT, value=border_color)\n",
    "        return bordered\n",
    "\n",
    "    #full preprocessing\n",
    "    def preprocess_complete(self, scale_factor=2.0, binarization_method='otsu', noise_removal_method='median',remove_border=True, add_border=True):\n",
    "         processed=self.gray_image.copy()\n",
    "         # Step 1: Rescale\n",
    "         processed=self.rescale_image(processed, scale_factor)\n",
    "            \n",
    "         # Step 2: Noise removal\n",
    "         processed=self.remove_noise(processed, method=noise_removal_method)\n",
    "         # Step 3: Remove borders if needed\n",
    "         if remove_border:\n",
    "             processed = self.remove_borders(processed)\n",
    "        \n",
    "         # Step 4: Binarization\n",
    "         processed = self.binarize_image(processed, method=binarization_method)\n",
    "        \n",
    "         # Step 5: Add borders if needed\n",
    "         if add_border:\n",
    "             processed = self.add_borders(processed)\n",
    "        \n",
    "         self.processed_image = processed\n",
    "         return processed\n",
    "        \n",
    "class OCRMetadataExtractor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'this', 'that', 'these', 'those'])\n",
    "    \n",
    "    def extract_metadata(self, extracted_text, file_path=None, total_pages=1):\n",
    "        metadata = {\n",
    "            'file_path': file_path,'extraction_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),'total_pages': total_pages,'total_words': len(extracted_text.split()), 'total_characters': len(extracted_text),\n",
    "        }\n",
    "        # Extract title\n",
    "        metadata['title']=self._extract_title(extracted_text)\n",
    "        \n",
    "        # Extract primary date\n",
    "        metadata['primary_date']=self._extract_primary_date(extracted_text)\n",
    "        \n",
    "        # Extract keywords\n",
    "        metadata['keywords']=self._extract_keywords(extracted_text)\n",
    "        \n",
    "        # Generate summary  \n",
    "        metadata['summary']=self._generate_summary(extracted_text)\n",
    "        \n",
    "        # Extract contact information\n",
    "        metadata['emails']=self._extract_emails(extracted_text)\n",
    "        metadata['phone_numbers']=self._extract_phone_numbers(extracted_text)\n",
    "        \n",
    "        # Document type\n",
    "        metadata['document_type']=self._classify_document_type(extracted_text)\n",
    "        return metadata\n",
    "    \n",
    "    # def _extract_title(self, text):\n",
    "    #     lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    #     if not lines:\n",
    "    #         return \"No title found\"\n",
    "        \n",
    "    #     # Get first substantial line\n",
    "    #     for line in lines[:3]:\n",
    "    #         if 10 < len(line) < 100:\n",
    "    #             return line\n",
    "        \n",
    "    #     return lines[0][:80] + \"...\" if len(lines[0]) > 80 else lines[0]\n",
    "\n",
    "    def _extract_title(self, text):\n",
    "        \"\"\"Enhanced title extraction with multiple strategies\"\"\"\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        if not lines:\n",
    "            return \"No title found\"\n",
    "        \n",
    "        # Strategy 1: Look for title-like patterns (common in documents)\n",
    "        title_patterns = [\n",
    "            r'^[A-Z][A-Za-z\\s]{10,80}$',  # Capitalized sentences\n",
    "            r'^[A-Z\\s]{5,}$',             # ALL CAPS titles\n",
    "            r'^\\d+\\.\\s*[A-Za-z\\s]{5,80}$', # Numbered titles\n",
    "        ]\n",
    "        \n",
    "        for line in lines[:5]:  # Check first 5 lines\n",
    "            for pattern in title_patterns:\n",
    "                if re.match(pattern, line) and 5 <= len(line) <= 100:\n",
    "                    return line\n",
    "        \n",
    "        # Strategy 2: Find the longest meaningful line in first few lines\n",
    "        candidates = []\n",
    "        for line in lines[:8]:  # Increased from 3 to 8 lines\n",
    "            # Remove common non-title elements\n",
    "            if not any(skip in line.lower() for skip in ['page', 'www.', 'http', '@', 'tel:', 'fax:']):\n",
    "                if 5 <= len(line) <= 150:  # Relaxed length constraints\n",
    "                    candidates.append((line, len(line)))\n",
    "        \n",
    "        if candidates:\n",
    "            # Return the longest candidate (likely to be title)\n",
    "            return max(candidates, key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Strategy 3: Fallback to first substantial line\n",
    "        for line in lines[:5]:\n",
    "            if 5 <= len(line) <= 200:  # More relaxed constraints\n",
    "                return line\n",
    "        \n",
    "        # Strategy 4: Last resort - return first line with reasonable length\n",
    "        return lines[0][:100] + \"...\" if len(lines[0]) > 100 else lines[0]\n",
    "\n",
    "    \n",
    "    def _extract_primary_date(self, text):\n",
    "        date_patterns = [\n",
    "            r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b',\n",
    "            r'\\b\\d{1,2}\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{2,4}\\b',\n",
    "            r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s+\\d{2,4}\\b',\n",
    "        ]\n",
    "        \n",
    "        for pattern in date_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _extract_keywords(self, text, max_keywords=10):\n",
    "        # Simple word extraction\n",
    "        words = re.findall(r'\\b[a-zA-Z]{4,}\\b', text.lower())\n",
    "        words = [word for word in words if word not in self.stop_words]\n",
    "        \n",
    "        # Count frequency and return top words\n",
    "        word_freq = Counter(words)\n",
    "        return [word for word, count in word_freq.most_common(max_keywords)]\n",
    "    \n",
    "    def _generate_summary(self, text, max_sentences=2):\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if len(s.strip()) > 30]\n",
    "        \n",
    "        if not sentences:\n",
    "            return \"No summary available\"\n",
    "        \n",
    "        if len(sentences) <= max_sentences:\n",
    "            return '. '.join(sentences) + '.'\n",
    "        \n",
    "        # Take first and last sentence\n",
    "        summary_sentences = [sentences[0]]\n",
    "        if len(sentences) > 1:\n",
    "            summary_sentences.append(sentences[-1])\n",
    "        \n",
    "        return '. '.join(summary_sentences) + '.'\n",
    "    \n",
    "    def _extract_emails(self, text):\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        emails = re.findall(email_pattern, text)\n",
    "        return emails[:3] if emails else []\n",
    "    \n",
    "    def _extract_phone_numbers(self, text):\n",
    "        phone_pattern = r'\\b(?:\\+?1[-.\\s]?)?\\(?[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4}\\b'\n",
    "        phones = re.findall(phone_pattern, text)\n",
    "        return phones[:3] if phones else []\n",
    "    \n",
    "    def _classify_document_type(self, text):\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        if any(word in text_lower for word in ['invoice', 'bill', 'payment']):\n",
    "            return 'Invoice/Bill'\n",
    "        elif any(word in text_lower for word in ['certificate', 'certification']):\n",
    "            return 'Certificate'\n",
    "        elif any(word in text_lower for word in ['contract', 'agreement']):\n",
    "            return 'Contract'\n",
    "        elif any(word in text_lower for word in ['report', 'analysis']):\n",
    "            return 'Report'\n",
    "        elif any(word in text_lower for word in ['letter', 'dear']):\n",
    "            return 'Letter'\n",
    "        else:\n",
    "            return 'General Document'             \n",
    "         \n",
    "         \n",
    "def process_and_view_ocr(images, show_images=False, save_results=False, output_dir=\"ocr_results\"):\n",
    "    preprocessor = OCRImageProcessor()\n",
    "    complete_text = \"\"\n",
    "    \n",
    "    if save_results:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        # Set image for preprocessing\n",
    "        preprocessor.set_image(image)\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        processed_image = preprocessor.preprocess_complete()\n",
    "        \n",
    "        # Extract text using OCR\n",
    "        ocr_text = pytesseract.image_to_string(processed_image)\n",
    "        \n",
    "        # Add to complete text\n",
    "        if len(images) > 1:\n",
    "            complete_text += f\"\\n{'='*60}\\n\"\n",
    "            complete_text += f\"PAGE {i+1}\\n\"\n",
    "            complete_text += f\"{'='*60}\\n\\n\"\n",
    "        complete_text += ocr_text + \"\\n\"\n",
    "        \n",
    "        # Save results if requested\n",
    "        if save_results:\n",
    "            cv2.imwrite(os.path.join(output_dir, f\"page_{i+1:03d}_processed.png\"), processed_image)\n",
    "            with open(os.path.join(output_dir, f\"page_{i+1:03d}_text.txt\"), 'w', encoding='utf-8') as f:\n",
    "                f.write(ocr_text)\n",
    "    \n",
    "    if save_results:\n",
    "        complete_text_path = os.path.join(output_dir, \"complete_extracted_text.txt\")\n",
    "        with open(complete_text_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(complete_text)\n",
    "    \n",
    "    return complete_text\n",
    "\n",
    "def universal_ocr(file_path, show_images=False, save_results=False, output_dir=\"ocr_results\"):\n",
    "    # Initialize processor\n",
    "    processor = UniversalOCRProcessor()\n",
    "    \n",
    "    # Detect file type\n",
    "    file_type = processor.detect_file_type(file_path)\n",
    "    \n",
    "    if file_type == 'unsupported':\n",
    "        raise ValueError(f\"Unsupported file type: {os.path.splitext(file_path)[1]}\")\n",
    "    \n",
    "    # Convert to images based on file type\n",
    "    images = []\n",
    "    if file_type == 'image':\n",
    "            img = Image.open(file_path)\n",
    "            images = [img]\n",
    "    elif file_type == 'pdf':\n",
    "        images = processor.pdf_to_images(file_path)\n",
    "    elif file_type == 'docx':\n",
    "        images = processor.docx_to_images(file_path)\n",
    "    elif file_type == 'text':\n",
    "        images = processor.txt_to_images(file_path)\n",
    "    \n",
    "    # Process images and extract text\n",
    "    extracted_text = process_and_view_ocr(\n",
    "        images, \n",
    "        show_images=show_images, \n",
    "        save_results=save_results, \n",
    "        output_dir=output_dir\n",
    "    )    \n",
    "    return extracted_text\n",
    "def universal_ocr_with_metadata(file_path, show_images=False, save_results=False, output_dir=\"ocr_results\"):\n",
    "    # Initialize processors\n",
    "    processor = UniversalOCRProcessor()\n",
    "    metadata_extractor = OCRMetadataExtractor()\n",
    "    \n",
    "    # Detect file type\n",
    "    file_type = processor.detect_file_type(file_path)\n",
    "    \n",
    "    if file_type == 'unsupported':\n",
    "        raise ValueError(f\"Unsupported file type: {os.path.splitext(file_path)[1]}\")\n",
    "    \n",
    "    # Convert to images based on file type  \n",
    "    images = []\n",
    "    if file_type == 'image':\n",
    "        img = Image.open(file_path)\n",
    "        images = [img]\n",
    "    elif file_type == 'pdf':\n",
    "        images = processor.pdf_to_images(file_path)\n",
    "    elif file_type == 'docx':\n",
    "        images = processor.docx_to_images(file_path)\n",
    "    elif file_type == 'text':\n",
    "        images = processor.txt_to_images(file_path)\n",
    "    \n",
    "    # Extract text\n",
    "    extracted_text = process_and_view_ocr(\n",
    "        images, \n",
    "        show_images=show_images, \n",
    "        save_results=save_results, \n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata = metadata_extractor.extract_metadata(\n",
    "        extracted_text, \n",
    "        file_path=file_path, \n",
    "        total_pages=len(images)\n",
    "    )\n",
    "    \n",
    "    return extracted_text, metadata\n",
    "\n",
    "def print_metadata(metadata):\n",
    "    print(\"DOCUMENT METADATA\")\n",
    "    \n",
    "    print(f\"File: {metadata.get('file_path', 'N/A')}\")\n",
    "    print(f\"Extracted: {metadata.get('extraction_date', 'N/A')}\")\n",
    "    print(f\"Type: {metadata.get('document_type', 'N/A')}\")\n",
    "    print(f\"Pages: {metadata.get('total_pages', 'N/A')}\")\n",
    "    print(f\"Words: {metadata.get('total_words', 'N/A')}\")\n",
    "    print(f\"Characters: {metadata.get('total_characters', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nTitle:\")\n",
    "    print(f\"{metadata.get('title', 'N/A')}\")\n",
    "    \n",
    "    if metadata.get('primary_date'):\n",
    "        print(f\"\\nDate Found: {metadata['primary_date']}\")\n",
    "    \n",
    "    if metadata.get('keywords'):\n",
    "        print(f\"\\nKeywords:\")\n",
    "        print(f\"   {', '.join(metadata['keywords'])}\")\n",
    "    \n",
    "    if metadata.get('summary'):\n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"{metadata['summary']}\")\n",
    "    \n",
    "    # Contact info\n",
    "    contact_info = []\n",
    "    if metadata.get('emails'):\n",
    "        contact_info.append(f\"{', '.join(metadata['emails'])}\")\n",
    "    if metadata.get('phone_numbers'):\n",
    "        contact_info.append(f\"{', '.join(metadata['phone_numbers'])}\")\n",
    "    \n",
    "    if contact_info:\n",
    "        print(f\"\\nContact Info:\")\n",
    "        for info in contact_info:\n",
    "            print(f\"{info}\")\n",
    "\n",
    "# Usage Examples\n",
    "# if __name__ == \"__main__\":\n",
    "#     file_path = \"Books_CFD.jpg\"\n",
    "    \n",
    "#     # Option 1: Just extract text\n",
    "#     # print(\"=\"*80)\n",
    "#     # print(\"OPTION 1: TEXT EXTRACTION ONLY\")\n",
    "#     # print(\"=\"*80)\n",
    "#     # result_text = universal_ocr(file_path, show_images=False, save_results=False)\n",
    "#     # print(result_text)\n",
    "    \n",
    "#     # Option 2: Extract text with metadata\n",
    "#     print(\"OPTION 2: TEXT EXTRACTION WITH METADATA\")\n",
    "#     # print(\"=\"*80)\n",
    "#     result_text, metadata = universal_ocr_with_metadata(\n",
    "#         file_path, \n",
    "#         show_images=False, \n",
    "#         save_results=False\n",
    "#     )\n",
    "    \n",
    "#     print(\"EXTRACTED TEXT:\")\n",
    "#     print(result_text)\n",
    "    \n",
    "#     # Print metadata\n",
    "#     print_metadata(metadata)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"Books_CFD.jpg\"\n",
    "    _, metadata = universal_ocr_with_metadata(file_path)\n",
    "    print_metadata(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc7f86-74a8-46aa-abcd-1fcf48b0bc91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
